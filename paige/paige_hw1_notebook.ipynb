{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Information\n",
    "\n",
    "Paige Haring\n",
    "\n",
    "peh40@pitt.edu\n",
    "\n",
    "September 1, 2017\n",
    "\n",
    "**Name:** Genesis Corpus\n",
    "\n",
    "**Authors:** N/A\n",
    "\n",
    "**Download URL:** http://www.nltk.org/nltk_data/\n",
    "\n",
    "**Makeup:** This corpus consists of 8 text files of written data, specifically, each file is a translation of the Book of Genesis. The 8 translations include: English- King James Bible, English- World English Bible, Finnish, French, German, lolcat, Portuguese, and Swedish. \n",
    "\n",
    "**License:** Public Domain\n",
    "\n",
    "**Noteworthy:** Of these translations, all represent a spoken human language except for lolcat, which is essentially a meme language \"spoken\" by cats on the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Assessment\n",
    "**Summary:** My code reads in the text files of the Genesis corpus using nltk's Plaintext Corpus Reader. At first, I had a lot of encoding problems. The nltk site where I downloaded the corpus didn't give me any information about what system was used for the files. I kept getting an error using utf-8, so after some research StackExchange told me to try out \n",
    "latin-1. I was able to actually read in the files now, BUT the characters being displayed for Swedish and Portuguese weren't the correct characters! I eventually found out that the files use three different encoding systems, and figured out how to pick and choose which one is used for which file when reading them in. I then displayed some basic stats- i.e. number of words/sentences total and per file. I also printed out the first line of each file to compare them. For my discovery question, I wanted to see how similar the vocabulary in each text was, so I looked at the top 5 most frequent words of each file. I excluded symbols and stopwords. I imported the stopwords corpus and used it as a filter. The only problem was that lolcats wasn't represented in the stopwords corpus, so I couldn't filter out words like \"teh\" (\"the\"). Interestingly, Jacob appears in the top five most common words list for all of the languages except for the KJV bible (and maybe lolcats?), and in *kind* of similar frequencies (185, 138, 202, 159, 180, 156). I also thought it was pretty interesting how different the frequency distributions of the two English versions were from each other. \n",
    "\n",
    "**Future Wish:** I'd like to learn how to align parallel data automatically to use for translation. That might not even be possible with this corpus despite it all being translations of Genesis, because each text file has a different number of sentences and words. I believe this is because not only are these bibles written by different authors in different languages, but they were also all written in different time periods. I would like to learn more about what makes parallel data \"good\" parallel data in general and if it would be possible to align this or make use of this corpus in some other way to aid in translation. I'd also like to learn what the correct way to handle my encoding problem is. If you have a dataset with a bunch of wacky characters, utf-8 can't handle it, and you don't know what was used, what is the best way to proceed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 files in the corpus!\n",
      "\n",
      "english-kjv.txt has 44764 words, 1467 sents\n",
      "english-web.txt has 44054 words, 2232 sents\n",
      "finnish.txt has 32520 words, 2160 sents\n",
      "french.txt has 46116 words, 2004 sents\n",
      "german.txt has 43941 words, 1900 sents\n",
      "lolcat.txt has 17074 words, 822 sents\n",
      "portuguese.txt has 45094 words, 1669 sents\n",
      "swedish.txt has 41705 words, 1386 sents\n",
      "There is a total of 315268 words and 13640 sents in the entire corpus.\n",
      "\n",
      "\n",
      "This is the first sentence of each file:\n",
      "english-kjv.txt: In the beginning God created the heaven and the earth.\n",
      "english-web.txt: In the beginning God created the heavens and the earth.\n",
      "finnish.txt: Alussa Jumala loi taivaan ja maan.\n",
      "french.txt: Au commencement, Dieu créa les cieux et la terre.\n",
      "german.txt: Am Anfang schuf Gott Himmel und Erde.\n",
      "lolcat.txt: Oh hai.\n",
      "portuguese.txt: No princípio, criou Deus os céus e a terra.\n",
      "swedish.txt: I begynnelsen skapade Gud himmel och jord.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "corpus_root = 'data/genesis'\n",
    "\n",
    "#The encoding for each of the text files was different! This was very annoying and I had\n",
    "#a hard time finding which file used what. I eventually found it in the code on this page!\n",
    "#http://www.nltk.org/_modules/nltk/corpus.html\n",
    "\n",
    "gens = PlaintextCorpusReader(corpus_root, r'.*\\.txt', encoding=[\n",
    "        ('finnish|french|german', 'latin_1'),\n",
    "        ('swedish', 'cp865'),\n",
    "        ('.*', 'utf_8')])\n",
    "\n",
    "print('There are',len(gens.fileids()),'files in the corpus!\\n')\n",
    "\n",
    "for f in gens.fileids():\n",
    "    print(f, 'has', len(gens.words(f)), 'words,', len(gens.sents(f)), 'sents')\n",
    "\n",
    "print('There is a total of', len(gens.words()), 'words and', len(gens.sents()), 'sents in the entire corpus.\\n')\n",
    "\n",
    "print(\"\\nThis is the first sentence of each file:\")\n",
    "\n",
    "for f in gens.fileids():\n",
    "    print(f+':',' '.join(gens.sents(f)[0]).replace(' .', '.').replace(' ,', ','))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovery Question\n",
    "What are the five most frequent words in each text, and how much do these lists differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english-kjv.txt : [('unto', 590), ('said', 476), ('thou', 272), ('thy', 267), ('thee', 257)]\n",
      "english-web.txt : [('said', 470), ('father', 270), ('God', 229), ('land', 195), ('Jacob', 185)]\n",
      "finnish.txt : [('sanoi', 324), ('Jumala', 180), ('Jaakob', 138), ('Herra', 123), ('Joosef', 114)]\n",
      "french.txt : [('fils', 317), ('Dieu', 230), ('Jacob', 202), ('père', 199), ('pays', 182)]\n",
      "german.txt : [('szlig', 677), ('sprach', 416), ('Gott', 207), ('h3', 200), ('Jakob', 159)]\n",
      "lolcat.txt : [('teh', 508), ('2', 189), ('wuz', 171), ('Ceiling', 166), ('Cat', 154)]\n",
      "portuguese.txt : [('terra', 349), ('Deus', 240), ('filhos', 209), ('pai', 200), ('Jacó', 180)]\n",
      "swedish.txt : [('sade', 400), ('skall', 283), ('Gud', 211), ('Jakob', 156), ('fader', 139)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#I want the five most frequent words excluding stopwords, so I'll bring in the stopwords corpus to be able to search for them.\n",
    "#All of the languages represented in the genesis corpus are represented in the stopwords corpus except, obviously, lolcats\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#This is for my numpy stuff\n",
    "top5 = []\n",
    "\n",
    "for f in gens.fileids():\n",
    "    fd = nltk.FreqDist(gens.words(f))\n",
    "    common = fd.most_common(55)\n",
    "    stops = set(stopwords.words())\n",
    "    common = [(x,y) for (x,y) in common if x.isalnum() and x.lower() not in stops]\n",
    "    top5.append(common[:5])\n",
    "    print(f, ':', common[:5])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy??\n",
    "What is the average frequency of the word Jacob in the texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[185, 138, 202, 159, 180, 156]\n",
      "The average frequency of Jacob is: 170.0\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "#I feel like I'm kind of cheating this way using startswith('Ja'). This seems better than a regex though?\n",
    "jake_freqs = [freq for lang in top5 for (word, freq) in lang if word.startswith('Ja')]\n",
    "mean = numpy.mean(jake_freqs)\n",
    "\n",
    "print(jake_freqs)\n",
    "print('The average frequency of Jacob is:', mean)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
