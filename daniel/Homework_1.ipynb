{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LING1340 Homework 1\n",
    "Name: **Daniel Zheng**\n",
    "\n",
    "Email: **daniel.zheng@pitt.edu**\n",
    "\n",
    "Due Date: **September 5, 2017**\n",
    "\n",
    "#### Dataset\n",
    "Large Movie Review Dataset from [here](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. *The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# useful libraries\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import re, glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading training data\n",
    "def load(filepath):\n",
    "    files = glob.glob(filepath)\n",
    "    raw = []\n",
    "    for file in files:\n",
    "        with open(file, encoding='utf-8') as f:\n",
    "            raw.append(f.read())\n",
    "    return raw\n",
    "train_neg_raw = load('data/aclImdb/train/neg/*')\n",
    "train_pos_raw = load('data/aclImdb/train/pos/*')\n",
    "test_neg_raw = load('data/aclImdb/test/neg/*')\n",
    "test_pos_raw = load('data/aclImdb/test/pos/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative training sample: [ 'Black and White film. Good photography. Believable characters. <br /><br />Just awful.<br /><br />I have wasted another perfect evening watching a film that other rated as \"worthy\" and \"very good.\" There is some good acting here and the back ground setting for the plot is good (more should have been done with this) but it is very slow to grow and never develops. It is totally bases on sex without much romance with much un needed nudity. More could have been done with the main characters. If you are looking for something to watch with you family this in not the movie and if not you will have trouble sitting through it. Though this film is long its only about 1 inch deep!']\n"
     ]
    }
   ],
   "source": [
    "print('Negative training sample:', np.random.choice(train_neg_raw, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive training sample:  [ '\"Mr. Harvey Lights a Candle\" is anchored by a brilliant performance by Timothy Spall.<br /><br />While we can predict that his titular morose, up tight teacher will have some sort of break down or catharsis based on some deep down secret from his past, how his emotions are unveiled is surprising. Spall\\'s range of feelings conveyed is quite moving and more than he usually gets to portray as part of the Mike Leigh repertory.<br /><br />While an expected boring school bus trip has only been used for comic purposes, such as on \"The Simpsons,\" this central situation of a visit to Salisbury Cathedral in Rhidian Brook\\'s script is well-contained and structured for dramatic purposes, and is almost formally divided into acts.<br /><br />We\\'re introduced to the urban British range of racially and religiously diverse kids (with their uniforms I couldn\\'t tell if this is a \"private\" or \"public\" school), as they gather \\x96 the rapping black kids, the serious South Asians and Muslims, the white bullies and mean girls \\x96 but conveyed quite naturally and individually. The young actors, some of whom I recognized from British TV such as \"Shameless,\" were exuberant in representing the usual range of junior high social pressures. Celia Imrie puts more warmth into the supervisor\\'s role than the martinets she usually has to play.<br /><br />A break in the trip leads to a transformative crisis for some while others remain amusingly oblivious. We think, like the teacher portrayed by Ben Miles of \"Coupling,\" that we will be spoon fed a didactic lesson about religious tolerance, but it\\'s much more about faith in people as well as God, which is why the BBC showed it in England at Easter time and BBC America showed it in the U.S. over Christmas.<br /><br />Nathalie Press, who was also so good in \"Summer of Love,\" has a key role in Mr. Harvey\\'s redemption that could have been played for movie-of-the-week preaching, but is touching as they reach out to each other in an unexpected way (unfortunately I saw their intense scene interrupted by commercials).<br /><br />While it is a bit heavy-handed in several times pointedly calling this road trip \"a pilgrimage,\" this quiet film was the best evocation of \"good will towards men\" than I\\'ve seen in most holiday-themed TV movies.']\n"
     ]
    }
   ],
   "source": [
    "print('Positive training sample: ', np.random.choice(train_pos_raw, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of Data\n",
    "There are train and test sets of data. Within `train` and `test`, there is a `neg` and `pos` folder each with 12,500 negative and positive samples. In the `train` folder there is also a folder called `unsup` with 50,000 examples for unsupervised learning.\n",
    "\n",
    "### Processing\n",
    "A lot of the data contains `<br>` tags from HTML, which will have to be cleaned up. I will weight using term frequency - inverse document frequency (tf-idf) and train a naive bayes classifier with sci-kit learn. For classifying movie reviews, I have a feeling adjectives and verbs are the most important, because words like \"terrible\", \"amazing\", \"thrilled\", and \"enjoyed\" just intuitively seem like they are more indicative of sentiment. For now, I will only keep those parts of speech, but it is definitely worth investigating using the other parts of speech.\n",
    "\n",
    "### Expected problems with approach\n",
    "A lot of the movie review data is going to be background information on the movies that probably won't be helpful for learning sentiment. Even though tf-idf will deemphasize many common words like \"is\", it might emphasize rare background info even more than common reviewing terms like \"terrible\", \"fantastic\", etc... Since background info terms should have low-frequency, it shouldn't make a huge difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function clean up data, taking out punctuation, numbers and special characters\n",
    "def clean(raw_input):\n",
    "    # tokenize and remove invalid characters\n",
    "    tokens = [word_tokenize(' '.join([x for x in string.split() if re.sub('[a-zA-Z0-9_.,!\"\\'-/]', '', x) == ''])) for string in raw_input]\n",
    "    # part of speech tags\n",
    "    review_pos = [pos_tag(x) for x in tokens]\n",
    "    # keep only adjectives and verbs for every review\n",
    "    cleaned = [' '.join([word for (word,pos) in phrase_pos if pos.find('JJ') != -1 or pos.find('V') != -1]) for phrase_pos in review_pos]\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# takes a while because tokenizing + pos_tag is slow\n",
    "train_neg = clean(train_neg_raw)\n",
    "train_pos = clean(train_pos_raw)\n",
    "test_neg = clean(test_neg_raw)\n",
    "test_pos = clean(test_pos_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stuffed shirt played veteran rare lead joins want find destroy secret creating falls lovely fetching uses meek get wooden marry Furious being used spurned uses get sure ai static lets uneventful talk-ridden slow Worse fails bring tedious blah acting insipid does help irascible enliven refreshing hammy drippy library obvious are pretty lousy unimpressive feeble fright is crummy uncredited staring great alleviate brain-numbing dull', \"other mentioned walked had been wanted stay have left 's think have been good is worst adapted 've seen starts goes say goes goes slow slow are interesting happen is depth underneath 's get is single entire need be entertained love good next add entire do care happens single start hoping die least be interesting watching inexplicable is strange unpredictable think be compelling 's quirky noir-esquire acting hard-boiled recognize talented miscast raising reading bizarre relevant slow\"]\n",
      "total negative training samples: 12500\n"
     ]
    }
   ],
   "source": [
    "# example of what the filtered data looks like\n",
    "print(train_neg[:2])\n",
    "print('total negative training samples:', len(train_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"see has struggling unable come new 's getting meets helps go do gets do tell are worthless soft-core safe marvels publishist great was did get finds continue tires wants know more does leave is best soft-core 've seen Check\", 'was expecting was surprised is light funny observed central were likable staggered clowned drug-centred was gentle subtle were witness sympathetic little old rural were captured was chosen scripted great detail have seen realistic alternative felt was light full expect difficult criticise thought last were lame have ended left overall unexpected']\n",
      "total positive training samples: 12500\n"
     ]
    }
   ],
   "source": [
    "print(train_pos[:2])\n",
    "print('total positive training samples:', len(train_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some visualizations\n",
    "from collections import Counter\n",
    "neg_word_counts = Counter()\n",
    "pos_word_counts = Counter()\n",
    "for neg, pos in zip(train_neg, train_pos):\n",
    "    neg_word_counts.update(word.strip('.,?!\"\\'').lower() for word in neg.split())\n",
    "    pos_word_counts.update(word.strip('.,?!\"\\'').lower() for word in pos.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 most common negative words: [('is', 51501), ('was', 27479), ('have', 15414), ('s', 15220), ('are', 15001), ('be', 14262), ('do', 9823), ('has', 7690), ('good', 7014), ('bad', 6696), ('were', 6258), ('had', 6237), ('did', 5831), ('does', 5354), ('see', 5278), ('been', 5056), ('get', 4814), ('make', 4447), ('made', 4284), ('other', 4211), ('think', 3489), ('being', 3339), ('more', 3237), ('first', 3211), ('know', 3205), ('seen', 3166), ('watch', 3072), ('much', 2999), ('say', 2859), ('many', 2825), ('ve', 2748), ('m', 2705), ('little', 2667), ('go', 2481), ('great', 2466), ('watching', 2452), ('going', 2259), ('re', 2205), ('worst', 2199), ('few', 2134), ('want', 2102), ('better', 2080), ('such', 2056), ('same', 2018), ('real', 2003), ('seems', 2000), ('got', 1980), ('least', 1972), ('most', 1957), ('only', 1953), ('funny', 1929), ('old', 1892), ('original', 1882), ('find', 1831), ('give', 1782), ('makes', 1780), ('best', 1745), ('gets', 1692), ('take', 1658), ('whole', 1577), ('thought', 1540), ('interesting', 1500), ('acting', 1489), ('am', 1476), ('trying', 1469), ('like', 1454), ('come', 1433), ('done', 1431), ('big', 1423), ('look', 1421), ('believe', 1390), ('saw', 1366), ('looks', 1347), ('last', 1341), ('poor', 1338), ('new', 1330), ('awful', 1330), ('making', 1323), ('terrible', 1302), ('own', 1292), ('stupid', 1272), ('looking', 1264), ('found', 1250), ('having', 1222), ('goes', 1219), ('put', 1219), ('sure', 1219), ('comes', 1209), ('main', 1200), ('said', 1184), ('supposed', 1178), ('young', 1153), ('seem', 1134), ('watched', 1131), ('boring', 1117), ('let', 1090), ('ending', 1090), ('worse', 1062), ('feel', 1055), ('left', 1053)]\n"
     ]
    }
   ],
   "source": [
    "print('100 most common negative words:',neg_word_counts.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 most common positive words: [('is', 58219), ('was', 22565), ('are', 15035), ('s', 13437), ('have', 12948), ('be', 12187), ('has', 9336), ('good', 7245), ('do', 6926), ('great', 5885), ('see', 5712), ('had', 5222), ('does', 4909), ('were', 4897), ('other', 4737), ('did', 4305), ('been', 4157), ('get', 4115), ('best', 3683), ('many', 3663), ('more', 3637), ('made', 3610), ('think', 3513), ('first', 3512), ('seen', 3317), ('being', 3164), ('make', 3104), ('little', 2969), ('watch', 2911), ('know', 2731), ('much', 2660), ('real', 2369), ('makes', 2362), ('say', 2331), ('most', 2253), ('ve', 2234), ('find', 2220), ('go', 2200), ('such', 2156), ('young', 2091), ('own', 1966), ('love', 1943), ('old', 1932), ('same', 1923), ('few', 1891), ('watching', 1815), ('m', 1787), ('bad', 1739), ('going', 1729), ('funny', 1680), ('new', 1670), ('got', 1639), ('saw', 1599), ('seems', 1582), ('like', 1561), ('played', 1548), ('take', 1541), ('excellent', 1532), ('give', 1517), ('want', 1513), ('big', 1499), ('re', 1495), ('last', 1494), ('gets', 1484), ('better', 1481), ('done', 1462), ('different', 1457), ('come', 1453), ('beautiful', 1392), ('interesting', 1391), ('thought', 1377), ('true', 1339), ('original', 1321), ('found', 1283), ('wonderful', 1263), ('am', 1251), ('comes', 1247), ('plays', 1238), ('takes', 1227), ('having', 1219), ('only', 1201), ('feel', 1173), ('goes', 1159), ('whole', 1139), ('ending', 1092), ('least', 1079), ('watched', 1078), ('sure', 1073), ('put', 1067), ('seeing', 1060), ('perfect', 1052), ('loved', 1052), ('look', 1041), ('nice', 1040), ('believe', 1036), ('i', 1031), ('gives', 1030), ('enjoy', 1025), ('acting', 1021), ('seem', 1015)]\n"
     ]
    }
   ],
   "source": [
    "print('100 most common positive words:',pos_word_counts.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency counts\n",
    "Looking at the processed data above, words like \"is\" and \"was\" are very common, as expected. The negative set also has negative words like \"awful\", \"terrible\", and \"stupid\", while the positive set has words like \"perfect\", \"excellent\", and \"beautiful\". This is a good sign! After applying tf-idf, it should be pretty easy for a classifier to determine sentiment.\n",
    "\n",
    "### Creating labels\n",
    "Train and test labels are assigned using `0` as the negative class and `1` as the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_neg + train_pos # concatenate for vectorizing\n",
    "train_labels = [0]*len(train_neg) + [1]*len(train_pos) # labels\n",
    "test = test_neg + test_pos\n",
    "test_labels = [0]*len(test_neg) + [1]*len(test_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# should be the same as CountVectorizer combined with TfidfTransformer\n",
    "tfidf = TfidfVectorizer()\n",
    "train_vectors = tfidf.fit_transform(train)\n",
    "# already fit to training set, so just transform\n",
    "test_vectors = tfidf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 30197)\n",
      "(25000, 30197)\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors.shape)\n",
    "print(test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90296\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB().fit(train_vectors, train_labels)\n",
    "print(np.mean(classifier.predict(train_vectors) == train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.308 % accuracy\n"
     ]
    }
   ],
   "source": [
    "predicted = classifier.predict(test_vectors)\n",
    "print(np.mean(predicted == test_labels)*100, '% accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "84.3% accuracy is pretty good! Definitely better than expected. Just for fun, I put together my own test set of movie review strings to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_test = [\"This movie was the worst. I hate it.\", \"Terrible acting. Negative, bland, uninteresting.\", \n",
    "               \"This movie was great, I really enjoyed the acting!\", \n",
    "               \"Amazing storyline, hilarious characters, and a shocking ending.\", \n",
    "               \"The vague plot was ridiculously boring, and put me to sleep.\"]\n",
    "custom_labels = [0,0,1,1,0]\n",
    "custom_test_vectors = tfidf.transform(clean(custom_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 % accuracy on custom test set\n",
      "[0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "custom_predictions = classifier.predict(custom_test_vectors)\n",
    "print(np.mean(custom_predictions == custom_labels)*100, '% accuracy on custom test set')\n",
    "print(custom_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "This code does a few things:\n",
    "1. Reads in movie review data so that each review is one string in a list\n",
    "2. Preprocesses, removing everything but adjectives and verbs within each review.\n",
    "3. Creates train and test tf-idf vectors\n",
    "4. Fits a naive-bayes classifier to the train vector\n",
    "5. Test on testing data\n",
    "So it looks like using tf-idf with a Multinomial Naive-Bayes classifier can pretty reliably guess binary sentiment of a movie review. This was by no means a comprehensive study, however. \n",
    "\n",
    "I would have liked to try other classifiers, and also experiment with some of the TfidfVectorizer parameters. Also, I would like to understand tf-idf and naive-bayes in a more in-depth manner, beyond just how to use them in code. \n",
    "\n",
    "Perhaps another interesting project to do with this data would be to generate positive and negative movie reviews, or to use autoencoders to do some unsupervised learning since they conveniently provided a folder of 50,000 unlabeled movie reviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
