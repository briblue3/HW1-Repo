{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Alicia Sigmon, als333@pitt.edu, 9/2/2017\n",
    "\n",
    "- Corpus: Pros and Cons\n",
    "- Author: Bing Liu\n",
    "- https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/pros_cons.zip\n",
    "- size: 746276, 2 txt files: 1381 KB and 1471 KB\n",
    "- format: corpus (2 files)\n",
    "- License: Creative Commons Attribution 4.0 International\n",
    "- The corpus's 2 files are of positive and negative reviews of technology products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Summary of Code:\n",
    "\n",
    "- First I read in the 2 files from the Pros and Cons corpus.\n",
    "- Next I calculated the number of files in the corpus using PlaintextCorpusReader\n",
    "    - I also practiced this using glob and commented it out - both methods are valid.\n",
    "- Then I printed 200 characters of each text to show what the data looked like.  \n",
    "- For my basic stats, I started by counting the number of entries for each text file.\n",
    "    - Because each entry began and ended with markers, I split the file using these parameters to create a list of entries, which I then could use to find the number of entries.      \n",
    "- I then removed the beginning and end markers so that they did not interfere with my word and sentence counts.\n",
    "- I used nltk to find word tokens, word types, and sentences in the texts.\n",
    "    -This data is messy because it comes from online forums. There are a lot of misspellings, missing / extra punctuation, and symbols that interfere with the word and sentence counts.  \n",
    "- For the discovery, I wanted to know what words and bigrams were most indicative of negative and postive reviews.\n",
    "    - I looked into frequencies using nltk. I did not remove the punctuation, so many of the most frequent words and bigrams included punctuation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future wish:\n",
    "\n",
    "I think it would be interesting to look at spelling errors in this corpus.\n",
    "Did people tend to make more errors in negative or positive reviews? \n",
    "I'm not sure how to look at spelling errors in a corpus like this \n",
    "because it is full of correct and incorrect spellings and extra symbols. \n",
    "We previously looked at minimum edit distance and spell correct in the Introduction to Computational Lingustics course, \n",
    "and I wonder how I would delve into a corpus looking for how often people make errors.\n",
    "\n",
    "Some issues with this idea are that it's likely that some people inputting their entries had spell \n",
    "checkers while others didn't, and they would have been different spell checkers regardless.\n",
    "If I were able to look at spelling errors in this corpus, it would therefore not be possible to generalize to \n",
    "erros in pro and con entries in general, but I could see in this corpus how many words in \n",
    "each category had spelling errors. To really look into spelling erros in pros and cons entries, \n",
    "the corpus would have to include people that had the same spell checker. \n",
    "\n",
    "Also, how would we compare the words to correctly spelled words? One idea I have is using \n",
    "a corpus of English words or maybe also a corpus of English slang to compare to the words in the Pros and Cons \n",
    "corpus. I'm not sure what the code would look like, and when I try to imagine it, it seems like each word type in the \n",
    "Pros and Cons corpus would have to be compared to words in English Words/Slang corpora to check for errors, \n",
    "which would take a very long time to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "file = open(\"data/IntegratedCons.txt\")\n",
    "constxt = file.read()\n",
    "file.close()\n",
    "\n",
    "file = open(\"data/IntegratedPros.txt\")\n",
    "prostxt = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 files in this corpus:\n",
      "\tIntegratedCons.txt\n",
      "\tIntegratedPros.txt\n"
     ]
    }
   ],
   "source": [
    "# Getting the fileids and counting the number of files\n",
    "# Option 1: Using PlaintextCorpusReader\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = 'data'\n",
    "corpus = PlaintextCorpusReader(corpus_root, '.*')\n",
    "#dir(corpus)\n",
    "#corpus.raw()[:500]\n",
    "print(\"There are \" + str(len(corpus.fileids())) + \" files in this corpus:\")\n",
    "for x in corpus.fileids():\n",
    "    print(\"\\t\"+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Using glob\n",
    "\n",
    "#import glob\n",
    "#files = glob.glob('data\\*.txt')\n",
    "#print(\"There are \" + str(len(files)) + \" files in this corpus:\")\n",
    "#for x in files:\n",
    "#    print(\"\\t\"+x.replace(\"data\\\\\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cons Preview:\n",
      "        <Cons>East batteries! On-off switch too easy to maneuver.</Cons>\n",
      "        <Cons>Eats...no, GULPS batteries</Cons>\n",
      "        <Cons>Awkward ergonomics, no optical viewfinder, short battery life, sl\n",
      "Pros Preview:\n",
      "        <Pros>Easy to use, economical!</Pros>\n",
      "        <Pros>Digital is where it's at...down with developing film!</Pros>\n",
      "        <Pros>Good image quality, 3x optical zoom, macro mode, inexpensive</Pro\n"
     ]
    }
   ],
   "source": [
    "# Looking at the data\n",
    "print(\"Cons Preview:\")\n",
    "print(constxt[:200])\n",
    "print(\"Pros Preview:\")\n",
    "print(prostxt[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Stats\n",
    "\n",
    "#nltk.download()\n",
    "import nltk\n",
    "\n",
    "#Splits the files by entry\n",
    "cons_split = constxt.replace(\"<Cons>\", \"\").split(\"</Cons>\\n\")\n",
    "pros_split = prostxt.replace(\"<Pros>\", \"\").split(\"</Pros>\\n\")\n",
    "\n",
    "#Removes the beginning and end markers of the entries for further anaylses\n",
    "new_constxt = constxt.replace(\"<Cons>\", \"\").replace(\"</Cons>\\n\", \"\")\n",
    "new_prostxt = prostxt.replace(\"<Pros>\", \"\").replace(\"</Pros>\\n\", \"\")\n",
    "\n",
    "# These include many things that are not words or not sentences\n",
    "    # Would be best to remove punction / symbols\n",
    "contoks = nltk.word_tokenize(new_constxt.lower())\n",
    "contypes = sorted(set(contoks))\n",
    "consents = nltk.sent_tokenize(new_constxt)\n",
    "protoks = nltk.word_tokenize(new_prostxt.lower())\n",
    "protypes = sorted(set(protoks))\n",
    "prosents = nltk.sent_tokenize(new_prostxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22936 con entries.\n",
      "There are 22941 pro entries.\n",
      "\n",
      "There are 45877 total entries in the Pros and Cons Corpus.\n",
      "\n",
      "There are 185655 con word tokens.\n",
      "There are 14487 con word types.\n",
      "There are 6850 con sentence tokens.\n",
      "\n",
      "There are 206167 pro word tokens.\n",
      "There are 12350 pro word types.\n",
      "There are 6956 pro sentence tokens.\n"
     ]
    }
   ],
   "source": [
    "#Printing basic stats from above - plus using numpy.sum() \n",
    "print(\"There are \" + str(len(cons_split)) + \" con entries.\")\n",
    "print(\"There are \" + str(len(pros_split)) + \" pro entries.\\n\")\n",
    "\n",
    "import numpy\n",
    "print(\"There are \" + str(numpy.sum([len(cons_split),len(pros_split)])) + \" total entries in the Pros and Cons Corpus.\\n\")\n",
    "\n",
    "print(\"There are \" + str(len(contoks)) + \" con word tokens.\")\n",
    "print(\"There are \" + str(len(contypes)) + \" con word types.\")\n",
    "print(\"There are \" + str(len(consents)) + \" con sentence tokens.\\n\")\n",
    "print(\"There are \" + str(len(protoks)) + \" pro word tokens.\")\n",
    "print(\"There are \" + str(len(protypes)) + \" pro word types.\")\n",
    "print(\"There are \" + str(len(prosents)) + \" pro sentence tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Con Words:\n",
      "(',', 14205)\n",
      "('.', 5763)\n",
      "('to', 3472)\n",
      "('a', 2925)\n",
      "('no', 2675)\n",
      "('not', 2633)\n",
      "('the', 2555)\n",
      "('battery', 2269)\n",
      "('is', 2196)\n",
      "('and', 2081)\n",
      "(';', 1966)\n",
      "('of', 1848)\n",
      "('!', 1575)\n",
      "('for', 1490)\n",
      "('life', 1484)\n",
      "(\"n't\", 1472)\n",
      "('slow', 1335)\n",
      "('in', 1298)\n",
      "('quality', 1241)\n",
      "('it', 1108)\n",
      "\n",
      "Top 20 Pro Words:\n",
      "(',', 32632)\n",
      "('.', 5739)\n",
      "('to', 5196)\n",
      "('quality', 5120)\n",
      "('easy', 4516)\n",
      "('and', 4490)\n",
      "('great', 4209)\n",
      "('use', 4071)\n",
      "('good', 3644)\n",
      "(';', 3154)\n",
      "('of', 2742)\n",
      "('price', 1985)\n",
      "('!', 1838)\n",
      "('features', 1813)\n",
      "('for', 1602)\n",
      "('battery', 1539)\n",
      "('small', 1532)\n",
      "('the', 1531)\n",
      "('a', 1464)\n",
      "('&', 1452)\n",
      "\n",
      "There are 20512 more pro word tokens than con word tokens.\n",
      "\n",
      "Here are examples of frequency perctanges:\n",
      "Con frequency of \",\": 0.07651288680617274\n",
      "Pro frequency of \",\": 0.15827945306474847\n",
      "Con frequency of \"battery\": 0.012221593816487571\n",
      "Pro frequency of \"battery\": 0.007464822207239762\n",
      "Con frequency of \"!\": 0.008483477417791064\n",
      "Pro frequency of \"!\": 0.008915102805007591\n"
     ]
    }
   ],
   "source": [
    "# Discovery Part 1: \n",
    "# What words are the most indicative of pro vs. con?\n",
    "\n",
    "#Takes the tokens and creates a frequency dictionary\n",
    "conFreq = nltk.FreqDist(contoks)\n",
    "proFreq = nltk.FreqDist(protoks)\n",
    "\n",
    "# Print most common words\n",
    "print(\"Top 20 Con Words:\")\n",
    "for x in conFreq.most_common(20):\n",
    "    print(x)\n",
    "print(\"\\nTop 20 Pro Words:\")\n",
    "for x in proFreq.most_common(20):\n",
    "    print(x)\n",
    "\n",
    "# Difference in total number of tokens\n",
    "print(\"\\nThere are \" + str(len(protoks)-len(contoks)) + \" more pro word tokens than con word tokens.\\n\")\n",
    "\n",
    "# Examples of some frequency percentages\n",
    "print(\"Here are examples of frequency perctanges:\")\n",
    "print(\"Con frequency of \\\",\\\":\", conFreq.freq(\",\"))\n",
    "print(\"Pro frequency of \\\",\\\":\", proFreq.freq(\",\"))\n",
    "\n",
    "print(\"Con frequency of \\\"battery\\\":\", conFreq.freq(\"battery\"))\n",
    "print(\"Pro frequency of \\\"battery\\\":\", proFreq.freq(\"battery\"))\n",
    "\n",
    "print(\"Con frequency of \\\"!\\\":\", conFreq.freq(\"!\"))\n",
    "print(\"Pro frequency of \\\"!\\\":\", proFreq.freq(\"!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Con Word Bigrams:\n",
      "(',', 'no') 1216\n",
      "('battery', 'life') 1188\n",
      "('a', 'little') 676\n",
      "('life', ',') 669\n",
      "('amp', ';') 593\n",
      "('&', 'amp') 557\n",
      "('hard', 'to') 542\n",
      "(',', 'poor') 470\n",
      "('!', '!') 431\n",
      "('does', \"n't\") 431\n",
      "(',', 'not') 425\n",
      "('a', 'bit') 407\n",
      "('.', 'no') 401\n",
      "(',', 'battery') 396\n",
      "(';', '#') 345\n",
      "('quality', ',') 332\n",
      "(',', 'slow') 328\n",
      "('could', 'be') 304\n",
      "('ca', \"n't\") 300\n",
      "(';', '&') 273\n",
      "\n",
      "Top 20 Pro Word Bigrams:\n",
      "(',', 'no') 1216\n",
      "('battery', 'life') 1188\n",
      "('a', 'little') 676\n",
      "('life', ',') 669\n",
      "('amp', ';') 593\n",
      "('&', 'amp') 557\n",
      "('hard', 'to') 542\n",
      "(',', 'poor') 470\n",
      "('!', '!') 431\n",
      "('does', \"n't\") 431\n",
      "(',', 'not') 425\n",
      "('a', 'bit') 407\n",
      "('.', 'no') 401\n",
      "(',', 'battery') 396\n",
      "(';', '#') 345\n",
      "('quality', ',') 332\n",
      "(',', 'slow') 328\n",
      "('could', 'be') 304\n",
      "('ca', \"n't\") 300\n",
      "(';', '&') 273\n"
     ]
    }
   ],
   "source": [
    "# Discovery Part 2:\n",
    "# What bigrams are most indicative of con or pro?\n",
    "\n",
    "bi_con = nltk.ngrams(contoks,2)\n",
    "bi_con_freq = nltk.FreqDist(bi_con)\n",
    "print(\"Top 20 Con Word Bigrams:\")\n",
    "for (x, y) in bi_con_freq.most_common(20):\n",
    "    print(x, y)\n",
    "\n",
    "bi_pro = nltk.ngrams(protoks,2)\n",
    "bi_pro_freq = nltk.FreqDist(bi_pro)\n",
    "print(\"\\nTop 20 Pro Word Bigrams:\")\n",
    "for (x, y) in bi_con_freq.most_common(20):\n",
    "    print(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
